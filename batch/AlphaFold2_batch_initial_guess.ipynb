{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RuizhiPeng/ColabFold/blob/main/batch/AlphaFold2_batch_initial_guess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4yBrceuFbf3"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/sokrypton/ColabFold/main/.github/ColabFold_Marv_Logo_Small.png\" height=\"200\" align=\"right\" style=\"height:240px\">\n",
    "\n",
    "##ColabFold v1.5.5: AlphaFold2 using MMseqs2\n",
    "\n",
    "Easy to use protein structure and complex prediction using [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) and [Alphafold2-multimer](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1). Sequence alignments/templates are generated through [MMseqs2](mmseqs.com) and [HHsearch](https://github.com/soedinglab/hh-suite). For more details, see <a href=\"#Instructions\">bottom</a> of the notebook, checkout the [ColabFold GitHub](https://github.com/sokrypton/ColabFold) and [Nature Protocols](https://www.nature.com/articles/s41596-024-01060-5).\n",
    "\n",
    "Old versions: [v1.4](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.4.0/AlphaFold2.ipynb), [v1.5.1](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.1/AlphaFold2.ipynb), [v1.5.2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.2/AlphaFold2.ipynb), [v1.5.3-patch](https://colab.research.google.com/github/sokrypton/ColabFold/blob/56c72044c7d51a311ca99b953a71e552fdc042e1/AlphaFold2.ipynb)\n",
    "\n",
    "[Mirdita M, Sch√ºtze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. ColabFold: Making protein folding accessible to all.\n",
    "*Nature Methods*, 2022](https://www.nature.com/articles/s41592-022-01488-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4nYk1mJRetuA"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1gkXfXHhFetx"
   },
   "outputs": [],
   "source": "#@title Configuration and Setup\n#@markdown ### Input/Output Settings\ninput_dir = '/content/drive/Othercomputers/prometheus/design/cirbp_86_108/af2_input_split1' #@param {type:\"string\"}\ninput_type = 'fasta' #@param [\"fasta\", \"pdb\"]\noutput_dir = '/content/drive/Othercomputers/prometheus/design/cirbp_86_108/af2_output' #@param {type:\"string\"}\n\n#@markdown ### Basic Model Settings\nnum_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\n#@markdown - specify how many of the top ranked structures to relax using amber\ntemplate_mode = \"none\" #@param [\"none\", \"pdb100\",\"custom\"]\n#@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100. `custom` - upload and search own templates\n\n#@markdown ### MSA Settings\nmsa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\npair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n\n#@markdown ### Advanced Model Settings\nmodel_type = \"alphafold2_multimer_v3\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\", \"alphafold2\"]\nnum_recycles = \"20\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"20\", \"24\", \"48\"]\nrecycle_early_stop_tolerance = \"0.5\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\nrelax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\npairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\ncalc_extra_ptm = False #@param {type:\"boolean\"}\n\n#@markdown ### Sampling Settings\nmax_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\nnum_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\nuse_dropout = False #@param {type:\"boolean\"}\n\n#@markdown ### Initial Guess Settings\nuse_initial_guess = True #@param {type:\"boolean\"}\ninitial_guess_dir = \"/content/drive/Othercomputers/prometheus/design/cirbp_86_108/logos_output\" #@param {type:\"string\"}\n#@markdown - `use_initial_guess` = soft initialization with desired coordinates (useful for binder design)\n\n#@markdown ### Save Settings\nsave_all = False #@param {type:\"boolean\"}\nsave_recycles = False #@param {type:\"boolean\"}\nsave_to_google_drive = True #@param {type:\"boolean\"}\ndpi = 200 #@param {type:\"integer\"}\n\n# =============================================================================\n# Helper Functions\n# =============================================================================\n\ndef build_fasta_from_pdb(pdb_files, input_dir):\n    import os\n    from Bio.PDB import PDBParser, PPBuilder\n\n    parser = PDBParser(QUIET=True)\n    ppb = PPBuilder()\n\n    fasta_files = []\n    for pdb_file in pdb_files:\n        structure_id = os.path.splitext(os.path.basename(pdb_file))[0]\n        fasta_file_path = os.path.join(input_dir, f'{structure_id}.fasta')\n\n        # Skip if FASTA already exists\n        if os.path.exists(fasta_file_path):\n            print(f\"  ‚è≠Ô∏è  Skipped: {structure_id}.fasta (already exists)\")\n            fasta_files.append(fasta_file_path)\n            continue\n\n        structure = parser.get_structure(structure_id, pdb_file)\n\n        # Extract sequences for each chain\n        chain_sequences = []\n        for model in structure:\n            for chain in model:\n                chain_id = chain.get_id()\n                chain_sequence = \"\"\n                for pp in ppb.build_peptides(chain):\n                    chain_sequence += str(pp.get_sequence())\n\n                if chain_sequence:\n                    chain_sequences.append((chain_id, chain_sequence))\n\n        # Write multi-chain FASTA file\n        if chain_sequences:\n            with open(fasta_file_path, 'w') as f:\n                for chain_id, sequence in chain_sequences:\n                    f.write(f\">chain_{chain_id}\\n\")\n                    f.write(f\"{sequence}\\n\")\n\n            fasta_files.append(fasta_file_path)\n            chain_info = \", \".join([f\"{chain_id}({len(seq)} aa)\" for chain_id, seq in chain_sequences])\n            print(f\"  ‚úì Created: {structure_id}.fasta [{chain_info}]\")\n\n    return fasta_files\n\ndef get_fasta_files(input_dir):\n    \"\"\"Efficiently get all FASTA files using os.scandir()\"\"\"\n    import os\n    fasta_files = []\n    fasta_extensions = ('.fasta', '.fa', '.fna')\n    \n    for entry in os.scandir(input_dir):\n        if entry.is_file() and entry.name.endswith(fasta_extensions):\n            fasta_files.append(entry.path)\n        elif entry.is_dir():\n            # Recursively scan subdirectories\n            fasta_files.extend(get_fasta_files(entry.path))\n    \n    return fasta_files\n\ndef get_sequence_from_fasta(file_path):\n    import os\n    base_name = os.path.splitext(os.path.basename(file_path))[0]\n    sequences = []\n    current_seq = \"\"\n    with open(file_path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.startswith(\">\"):\n                if current_seq:\n                    sequences.append(current_seq)\n                    current_seq = \"\"\n            else:\n                current_seq += line\n        if current_seq:\n            sequences.append(current_seq)\n    combined_sequence = ':'.join(sequences)\n\n    # Check legit sequence\n    valid_aa = set(\"ACDEFGHIKLMNPQRSTVWY\")\n    if not all(residue in valid_aa for residue in combined_sequence.replace(\":\", \"\")):\n        print(f\"Warning: Sequence in {file_path} contains non-standard amino acids. Skipping this file.\")\n        return {}\n    else:\n        return {base_name: combined_sequence}\n\ndef add_hash(x, y):\n    import hashlib\n    return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]\n\ndef load_completion_cache(output_dir):\n    \"\"\"Load completion cache from JSON file\"\"\"\n    import os\n    import json\n    cache_file = os.path.join(output_dir, '.completion_cache.json')\n    if os.path.exists(cache_file):\n        try:\n            with open(cache_file, 'r') as f:\n                return json.load(f)\n        except:\n            return {}\n    return {}\n\ndef save_completion_cache(output_dir, cache):\n    \"\"\"Save completion cache to JSON file\"\"\"\n    import os\n    import json\n    cache_file = os.path.join(output_dir, '.completion_cache.json')\n    try:\n        with open(cache_file, 'w') as f:\n            json.dump(cache, f, indent=2)\n    except:\n        pass\n\ndef is_sequence_completed(job_dir, cache=None, use_cache=True):\n    \"\"\"Check if a .done.txt file exists in the job directory (optimized with glob)\"\"\"\n    import os\n    import glob\n    \n    if not os.path.exists(job_dir):\n        return False\n    \n    # Check cache first if available\n    if use_cache and cache is not None and job_dir in cache:\n        return cache[job_dir]\n    \n    # Use glob pattern which is more efficient than listdir\n    result = bool(glob.glob(os.path.join(job_dir, '*.done.txt')))\n    \n    # Update cache if provided\n    if cache is not None:\n        cache[job_dir] = result\n    \n    return result\n\ndef is_queries_csv_ready(job_dir, jobname):\n    \"\"\"Check if queries CSV file exists and has content (optimized)\"\"\"\n    import os\n    queries_path = os.path.join(job_dir, f\"{jobname}.csv\")\n    # Just check existence and non-zero size for efficiency\n    return os.path.exists(queries_path) and os.path.getsize(queries_path) > 0\n\ndef build_existing_dirs_mapping(output_dir):\n    \"\"\"Pre-compute all existing output directories for fast lookup\"\"\"\n    import os\n    existing_dirs = {}\n    if os.path.exists(output_dir):\n        try:\n            for entry in os.scandir(output_dir):\n                if entry.is_dir():\n                    existing_dirs[entry.name] = entry.path\n        except:\n            pass\n    return existing_dirs\n\n# =============================================================================\n# Process Input Files\n# =============================================================================\n\nimport os\nimport re\nfrom sys import version_info\n\npython_version = f\"{version_info.major}.{version_info.minor}\"\nuse_amber = num_relax > 0\n\n# Handle PDB input - extract sequences and create FASTA files\nif input_type == 'pdb':\n    print(\"=\" * 70)\n    print(\"STEP 1: Converting PDB files to FASTA format\")\n    print(\"=\" * 70)\n\n    os.system(\"pip install -q --no-warn-conflicts biopython\")\n    pdb_files = []\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith(\".pdb\"):\n                pdb_files.append(os.path.join(root, file))\n\n    if not pdb_files:\n        raise ValueError(f\"No PDB files found in the directory: {input_dir}\")\n\n    print(f\"Found {len(pdb_files)} PDB files in {input_dir}\")\n    print(\"Extracting sequences and creating FASTA files...\")\n    fasta_files_created = build_fasta_from_pdb(pdb_files, input_dir)\n    print(f\"‚úì Processed {len(pdb_files)} PDB files ({len(fasta_files_created)} FASTA files ready)\\n\")\nelse:\n    print(\"=\" * 70)\n    print(\"STEP 1: Skipping PDB conversion (input_type='fasta')\")\n    print(\"=\" * 70 + \"\\n\")\n\n# Get FASTA files from input directory\nprint(\"=\" * 70)\nprint(\"STEP 2: Discovering FASTA files\")\nprint(\"=\" * 70)\n\nfasta_files = get_fasta_files(input_dir)\nif not fasta_files:\n    raise ValueError(f\"No FASTA files found in the directory: {input_dir}\")\nprint(f\"‚úì Found {len(fasta_files)} FASTA files in {input_dir}\\n\")\n\n# Create output directory\nos.makedirs(output_dir, exist_ok=True)\n\n# Load completion cache and build existing directories mapping\nprint(\"=\" * 70)\nprint(\"STEP 3: Processing sequences and checking completion status\")\nprint(\"=\" * 70)\n\nprint(\"Loading completion cache...\")\ncompletion_cache = load_completion_cache(output_dir)\nprint(f\"‚úì Cache loaded with {len(completion_cache)} entries\")\n\nprint(\"Building existing directories mapping...\")\nexisting_dirs = build_existing_dirs_mapping(output_dir)\nprint(f\"‚úì Found {len(existing_dirs)} existing output directories\\n\")\n\nsequences_dict = {}\nfasta_file_mapping = {}\nskipped_sequences = []\n\nfor fasta_file in fasta_files:\n    sequence_data = get_sequence_from_fasta(fasta_file)\n    if sequence_data:\n        for jobname, sequence in sequence_data.items():\n            clean_sequence = \"\".join(sequence.split())\n            base_jobname = \"\".join(jobname.split())\n            base_jobname = re.sub(r'\\W+', '', base_jobname)\n            hashed_jobname = add_hash(base_jobname, clean_sequence)\n\n            # Fast lookup using pre-built mapping\n            if hashed_jobname in existing_dirs:\n                job_dir = existing_dirs[hashed_jobname]\n            else:\n                job_dir = os.path.join(output_dir, hashed_jobname)\n            \n            # Check completion with cache\n            if is_sequence_completed(job_dir, cache=completion_cache, use_cache=True):\n                print(f\"‚è≠Ô∏è  {jobname} ‚Üí {hashed_jobname} [COMPLETED - has .done.txt]\")\n                skipped_sequences.append(jobname)\n            else:\n                sequences_dict[jobname] = sequence\n                fasta_file_mapping[jobname] = fasta_file\n                print(f\"üìã {jobname} ‚Üí {hashed_jobname} [QUEUED]\")\n\n# Save updated cache\nsave_completion_cache(output_dir, completion_cache)\nprint(f\"\\n‚úì Status: {len(sequences_dict)} sequences queued, {len(skipped_sequences)} already completed\\n\")\n\n# =============================================================================\n# Prepare Batch Jobs\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"STEP 4: Preparing batch job directories and query files\")\nprint(\"=\" * 70)\n\nbatch_jobs = []\nqueries_created = 0\nqueries_skipped = 0\n\nfor current_jobname, current_query_sequence in sequences_dict.items():\n    clean_query_sequence = \"\".join(current_query_sequence.split())\n\n    base_jobname = \"\".join(current_jobname.split())\n    base_jobname = re.sub(r'\\W+', '', base_jobname)\n    hashed_jobname = add_hash(base_jobname, clean_query_sequence)\n    final_jobname = hashed_jobname\n\n    job_dir = os.path.join(output_dir, final_jobname)\n    os.makedirs(job_dir, exist_ok=True)\n\n    queries_path = os.path.join(job_dir, f\"{final_jobname}.csv\")\n\n    # Check if queries CSV already exists and is valid (optimized)\n    if is_queries_csv_ready(job_dir, final_jobname):\n        print(f\"  ‚è≠Ô∏è  {final_jobname}.csv (already exists)\")\n        queries_skipped += 1\n    else:\n        # Create queries CSV\n        with open(queries_path, \"w\") as text_file:\n            text_file.write(f\"id,sequence\\n{final_jobname},{clean_query_sequence}\")\n        print(f\"  ‚úì Created {final_jobname}.csv (length: {len(clean_query_sequence.replace(':', ''))} aa)\")\n        queries_created += 1\n\n    fasta_file_path = fasta_file_mapping.get(current_jobname, \"\")\n    fasta_basename = os.path.splitext(os.path.basename(fasta_file_path))[0] if fasta_file_path else \"\"\n\n    batch_jobs.append({\n        'jobname': final_jobname,\n        'original_jobname': current_jobname,\n        'fasta_basename': fasta_basename,\n        'query_sequence': clean_query_sequence,\n        'queries_path': queries_path,\n        'job_dir': job_dir\n    })\n\nprint(f\"\\n‚úì Query files: {queries_created} created, {queries_skipped} already existed\")\nprint(f\"‚úì Total batch jobs ready: {len(batch_jobs)}\\n\")\n\n# Process parameters\nprint(\"=\" * 70)\nprint(\"STEP 5: Processing parameters\")\nprint(\"=\" * 70)\n\nnum_recycles = None if num_recycles == \"auto\" else int(num_recycles)\nrecycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\nif max_msa == \"auto\":\n    max_msa = None\n\n# Template settings\nif template_mode == \"pdb100\":\n    use_templates = True\n    custom_template_path = None\nelse:\n    custom_template_path = None\n    use_templates = False\n\nprint(f\"  Model type: {model_type}\")\nprint(f\"  MSA mode: {msa_mode}\")\nprint(f\"  Num recycles: {num_recycles if num_recycles is not None else 'auto'}\")\nprint(f\"  Num seeds: {num_seeds}\")\nprint(f\"  Use templates: {use_templates}\")\nprint(f\"  Num relax: {num_relax}\\n\")\n\n# Build initial guess mapping if enabled\nprint(\"=\" * 70)\nprint(\"STEP 6: Building initial guess mapping\")\nprint(\"=\" * 70)\n\ninitial_guess_mapping = {}\nif use_initial_guess and initial_guess_dir:\n    if os.path.exists(initial_guess_dir):\n        # Collect all PDB files with their basenames\n        pdb_files = {}\n        for file in os.listdir(initial_guess_dir):\n            if file.endswith('.pdb') or file.endswith('.cif'):\n                basename = os.path.splitext(file)[0]\n                pdb_files[basename] = os.path.join(initial_guess_dir, file)\n\n        print(f\"Found {len(pdb_files)} PDB/CIF files in initial guess directory\")\n\n        # Match FASTA files to PDB files using prefix matching\n        exact_matches = 0\n        prefix_matches = 0\n        no_matches = 0\n\n        for fasta_file in fasta_files:\n            fasta_basename = os.path.splitext(os.path.basename(fasta_file))[0]\n            matched = False\n\n            # First try exact match\n            if fasta_basename in pdb_files:\n                initial_guess_mapping[fasta_basename] = pdb_files[fasta_basename]\n                print(f\"  ‚úì Exact match: {fasta_basename} ‚Üí {os.path.basename(pdb_files[fasta_basename])}\")\n                exact_matches += 1\n                matched = True\n            else:\n                # Try prefix matching: find PDB whose name is a prefix of the FASTA name\n                for pdb_basename, pdb_path in pdb_files.items():\n                    if fasta_basename.startswith(pdb_basename):\n                        initial_guess_mapping[fasta_basename] = pdb_path\n                        print(f\"  ‚úì Prefix match: {fasta_basename} ‚Üí {os.path.basename(pdb_path)}\")\n                        prefix_matches += 1\n                        matched = True\n                        break\n\n            if not matched:\n                print(f\"  ‚ö†Ô∏è  No match: {fasta_basename}\")\n                no_matches += 1\n\n        print(f\"\\n‚úì Initial guess summary:\")\n        print(f\"  - Exact matches: {exact_matches}\")\n        print(f\"  - Prefix matches: {prefix_matches}\")\n        print(f\"  - No matches: {no_matches}\")\n        print(f\"  - Total mapped: {len(initial_guess_mapping)}/{len(fasta_files)}\\n\")\n    else:\n        print(f\"‚ö†Ô∏è  Warning: Initial guess directory not found: {initial_guess_dir}\")\n        use_initial_guess = False\nelif use_initial_guess and not initial_guess_dir:\n    print(\"‚ö†Ô∏è  Warning: use_initial_guess enabled but no initial_guess_dir provided\")\n    use_initial_guess = False\nelse:\n    print(\"Initial guess disabled\\n\")\n\nprint(\"=\" * 70)\nprint(\"‚úì CONFIGURATION COMPLETE - Ready to run predictions\")\nprint(\"=\" * 70)\nprint(f\"\\nSummary:\")\nprint(f\"  - Total FASTA files: {len(fasta_files)}\")\nprint(f\"  - Sequences to process: {len(batch_jobs)}\")\nprint(f\"  - Sequences already completed: {len(skipped_sequences)}\")\nprint(f\"  - Initial guess enabled: {use_initial_guess}\")\nif use_initial_guess:\n    print(f\"  - Initial guess mappings: {len(initial_guess_mapping)}\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kOblAo-xetgx"
   },
   "outputs": [],
   "source": [
    "#@title Install All Dependencies\n",
    "#@markdown This cell installs ColabFold, Amber (if needed), and HHsuite (if needed)\n",
    "\n",
    "%%time\n",
    "import os\n",
    "\n",
    "USE_AMBER = use_amber\n",
    "USE_TEMPLATES = use_templates\n",
    "PYTHON_VERSION = python_version\n",
    "\n",
    "# Install ColabFold\n",
    "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
    "    print(\"Installing ColabFold...\")\n",
    "    os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
    "    if os.environ.get('TPU_NAME', False) != False:\n",
    "        os.system(\"pip uninstall -y jax jaxlib\")\n",
    "        os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
    "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
    "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
    "    # Fix TF crash\n",
    "    os.system(\"rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\")\n",
    "    os.system(\"touch COLABFOLD_READY\")\n",
    "    print(\"‚úì ColabFold installed\")\n",
    "\n",
    "# Install Conda if needed\n",
    "if USE_AMBER or USE_TEMPLATES:\n",
    "    if not os.path.isfile(\"CONDA_READY\"):\n",
    "        print(\"Installing Conda...\")\n",
    "        os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
    "        os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
    "        os.system(\"mamba config --set auto_update_conda false\")\n",
    "        os.system(\"touch CONDA_READY\")\n",
    "        print(\"‚úì Conda installed\")\n",
    "\n",
    "# Install HHsuite and/or Amber\n",
    "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "    print(\"Installing HHsuite and Amber...\")\n",
    "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "    os.system(\"touch HH_READY\")\n",
    "    os.system(\"touch AMBER_READY\")\n",
    "    print(\"‚úì HHsuite and Amber installed\")\n",
    "else:\n",
    "    if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
    "        print(\"Installing HHsuite...\")\n",
    "        os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
    "        os.system(\"touch HH_READY\")\n",
    "        print(\"‚úì HHsuite installed\")\n",
    "    if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "        print(\"Installing Amber...\")\n",
    "        os.system(f\"mamba install -y -c conda-forge openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "        os.system(\"touch AMBER_READY\")\n",
    "        print(\"‚úì Amber installed\")\n",
    "\n",
    "print(\"\\n‚úì All dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AzIKiDiCaHAn"
   },
   "outputs": [],
   "source": [
    "#@title Run Prediction\n",
    "display_images = False #@param {type:\"boolean\"}\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from Bio import BiopythonDeprecationWarning\n",
    "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
    "from pathlib import Path\n",
    "from colabfold.download import download_alphafold_params, default_data_dir\n",
    "from colabfold.utils import setup_logging\n",
    "from colabfold.batch import get_queries, run, set_model_type\n",
    "from colabfold.plot import plot_msa_v2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check for K80 GPU\n",
    "try:\n",
    "    K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
    "except:\n",
    "    K80_chk = \"0\"\n",
    "    pass\n",
    "\n",
    "if \"1\" in K80_chk:\n",
    "    print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
    "    if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
    "        del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
    "    if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
    "        del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
    "\n",
    "from colabfold.colabfold import plot_protein\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add pdbfixer to path if using amber\n",
    "if use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
    "    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
    "\n",
    "def input_features_callback(input_features):\n",
    "    if display_images:\n",
    "        plot_msa_v2(input_features)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def prediction_callback(protein_obj, length, prediction_result, input_features, mode):\n",
    "    model_name, relaxed = mode\n",
    "    if not relaxed:\n",
    "        if display_images:\n",
    "            fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "# Download AlphaFold parameters once for all jobs\n",
    "for i, job in enumerate(batch_jobs):\n",
    "    current_queries_path = job['queries_path']\n",
    "    queries, is_complex = get_queries(current_queries_path)\n",
    "    model_type = set_model_type(is_complex, model_type)\n",
    "    break\n",
    "\n",
    "download_alphafold_params(model_type, Path(\".\"))\n",
    "\n",
    "# Batch processing: run prediction for each sequence\n",
    "print(f\"Starting batch processing for {len(batch_jobs)} sequences...\")\n",
    "batch_results = []\n",
    "\n",
    "for i, job in enumerate(batch_jobs):\n",
    "    current_jobname = job['jobname']\n",
    "    current_query_sequence = job['query_sequence']\n",
    "    current_queries_path = job['queries_path']\n",
    "    current_job_dir = job['job_dir']\n",
    "    fasta_basename = job.get('fasta_basename', '')\n",
    "\n",
    "    print(f\"\\n=== Processing {i+1}/{len(batch_jobs)}: {current_jobname} ===\")\n",
    "\n",
    "    # Setup MSA file path based on mode\n",
    "    if \"mmseqs2\" in msa_mode:\n",
    "        a3m_file = os.path.join(current_job_dir, f\"{current_jobname}.a3m\")\n",
    "    else:  # single_sequence mode\n",
    "        a3m_file = os.path.join(current_job_dir, f\"{current_jobname}.single_sequence.a3m\")\n",
    "        with open(a3m_file, \"w\") as text_file:\n",
    "            text_file.write(f\">1\\n{current_query_sequence}\")\n",
    "\n",
    "    # Setup logging\n",
    "    log_filename = os.path.join(current_job_dir, \"log.txt\")\n",
    "    setup_logging(Path(log_filename))\n",
    "\n",
    "    # Get queries and model type\n",
    "    queries, is_complex = get_queries(current_queries_path)\n",
    "    model_type = set_model_type(is_complex, model_type)\n",
    "\n",
    "    if \"multimer\" in model_type and max_msa is not None:\n",
    "        use_cluster_profile = False\n",
    "    else:\n",
    "        use_cluster_profile = True\n",
    "\n",
    "    # Setup initial guess\n",
    "    current_initial_guess = None\n",
    "    if use_initial_guess and fasta_basename:\n",
    "        if fasta_basename in initial_guess_mapping:\n",
    "            current_initial_guess = initial_guess_mapping[fasta_basename]\n",
    "            print(f\"Using initial guess from: {os.path.basename(current_initial_guess)}\")\n",
    "        else:\n",
    "            print(f\"Warning: No initial guess PDB found for {fasta_basename}\")\n",
    "\n",
    "    # Run prediction\n",
    "    try:\n",
    "        results = run(\n",
    "            queries=queries,\n",
    "            result_dir=current_job_dir,\n",
    "            use_templates=use_templates,\n",
    "            custom_template_path=custom_template_path,\n",
    "            num_relax=num_relax,\n",
    "            msa_mode=msa_mode,\n",
    "            model_type=model_type,\n",
    "            num_models=5,\n",
    "            num_recycles=num_recycles,\n",
    "            relax_max_iterations=relax_max_iterations,\n",
    "            recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
    "            num_seeds=num_seeds,\n",
    "            use_dropout=use_dropout,\n",
    "            model_order=[1,2,3,4,5],\n",
    "            initial_guess=current_initial_guess,\n",
    "            is_complex=is_complex,\n",
    "            data_dir=Path(\".\"),\n",
    "            keep_existing_results=False,\n",
    "            rank_by=\"auto\",\n",
    "            pair_mode=pair_mode,\n",
    "            pairing_strategy=pairing_strategy,\n",
    "            stop_at_score=float(100),\n",
    "            prediction_callback=prediction_callback,\n",
    "            dpi=dpi,\n",
    "            zip_results=False,\n",
    "            save_all=save_all,\n",
    "            max_msa=max_msa,\n",
    "            use_cluster_profile=use_cluster_profile,\n",
    "            input_features_callback=input_features_callback,\n",
    "            save_recycles=save_recycles,\n",
    "            user_agent=\"colabfold/google-colab-main\",\n",
    "            calc_extra_ptm=calc_extra_ptm,\n",
    "        )\n",
    "\n",
    "        # Create result zip for this job\n",
    "        results_zip = os.path.join(current_job_dir, f\"{current_jobname}.result.zip\")\n",
    "        os.system(f\"cd {current_job_dir} && zip -r {current_jobname}.result.zip .\")\n",
    "\n",
    "        batch_results.append({\n",
    "            'jobname': current_jobname,\n",
    "            'results': results,\n",
    "            'zip_path': results_zip,\n",
    "            'job_dir': current_job_dir\n",
    "        })\n",
    "\n",
    "        print(f\"‚úì Completed {current_jobname}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed {current_jobname}: {str(e)}\")\n",
    "        batch_results.append({\n",
    "            'jobname': current_jobname,\n",
    "            'results': None,\n",
    "            'zip_path': None,\n",
    "            'job_dir': current_job_dir,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "successful_count = len([r for r in batch_results if r.get('results')])\n",
    "failed_count = len([r for r in batch_results if r.get('error')])\n",
    "print(f\"\\nBatch processing completed: {successful_count} successful, {failed_count} failed\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}