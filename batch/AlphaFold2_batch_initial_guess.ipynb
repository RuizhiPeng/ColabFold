{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RuizhiPeng/ColabFold/blob/main/batch/AlphaFold2_batch_initial_guess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4yBrceuFbf3"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/sokrypton/ColabFold/main/.github/ColabFold_Marv_Logo_Small.png\" height=\"200\" align=\"right\" style=\"height:240px\">\n",
    "\n",
    "##ColabFold v1.5.5: AlphaFold2 using MMseqs2\n",
    "\n",
    "Easy to use protein structure and complex prediction using [AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) and [Alphafold2-multimer](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1). Sequence alignments/templates are generated through [MMseqs2](mmseqs.com) and [HHsearch](https://github.com/soedinglab/hh-suite). For more details, see <a href=\"#Instructions\">bottom</a> of the notebook, checkout the [ColabFold GitHub](https://github.com/sokrypton/ColabFold) and [Nature Protocols](https://www.nature.com/articles/s41596-024-01060-5).\n",
    "\n",
    "Old versions: [v1.4](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.4.0/AlphaFold2.ipynb), [v1.5.1](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.1/AlphaFold2.ipynb), [v1.5.2](https://colab.research.google.com/github/sokrypton/ColabFold/blob/v1.5.2/AlphaFold2.ipynb), [v1.5.3-patch](https://colab.research.google.com/github/sokrypton/ColabFold/blob/56c72044c7d51a311ca99b953a71e552fdc042e1/AlphaFold2.ipynb)\n",
    "\n",
    "[Mirdita M, Sch√ºtze K, Moriwaki Y, Heo L, Ovchinnikov S, Steinegger M. ColabFold: Making protein folding accessible to all.\n",
    "*Nature Methods*, 2022](https://www.nature.com/articles/s41592-022-01488-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4nYk1mJRetuA"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1gkXfXHhFetx"
   },
   "outputs": [],
   "source": [
    "#@title Configuration and Setup\n",
    "#@markdown ### Input/Output Settings\n",
    "input_dir = '/content/drive/Othercomputers/prometheus/design/cirbp_86_108/af2_input_split1' #@param {type:\"string\"}\n",
    "input_type = 'fasta' #@param [\"fasta\", \"pdb\"]\n",
    "output_dir = '/content/drive/Othercomputers/prometheus/design/cirbp_86_108/af2_output' #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Basic Model Settings\n",
    "num_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\n",
    "#@markdown - specify how many of the top ranked structures to relax using amber\n",
    "template_mode = \"none\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
    "#@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100. `custom` - upload and search own templates\n",
    "\n",
    "#@markdown ### MSA Settings\n",
    "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
    "pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
    "#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
    "\n",
    "#@markdown ### Advanced Model Settings\n",
    "model_type = \"alphafold2_multimer_v3\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\", \"alphafold2\"]\n",
    "num_recycles = \"20\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"20\", \"24\", \"48\"]\n",
    "recycle_early_stop_tolerance = \"0.5\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
    "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
    "pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
    "calc_extra_ptm = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### Sampling Settings\n",
    "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
    "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
    "use_dropout = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### Initial Guess Settings\n",
    "use_initial_guess = True #@param {type:\"boolean\"}\n",
    "initial_guess_dir = \"/content/drive/Othercomputers/prometheus/design/cirbp_86_108/logos_output\" #@param {type:\"string\"}\n",
    "#@markdown - `use_initial_guess` = soft initialization with desired coordinates (useful for binder design)\n",
    "\n",
    "#@markdown ### Save Settings\n",
    "save_all = False #@param {type:\"boolean\"}\n",
    "save_recycles = False #@param {type:\"boolean\"}\n",
    "save_to_google_drive = True #@param {type:\"boolean\"}\n",
    "dpi = 200 #@param {type:\"integer\"}\n",
    "\n",
    "# =============================================================================\n",
    "# Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def build_fasta_from_pdb(pdb_files, input_dir):\n",
    "    import os\n",
    "    from Bio.PDB import PDBParser, PPBuilder\n",
    "\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    ppb = PPBuilder()\n",
    "\n",
    "    fasta_files = []\n",
    "    for pdb_file in pdb_files:\n",
    "        structure_id = os.path.splitext(os.path.basename(pdb_file))[0]\n",
    "        fasta_file_path = os.path.join(input_dir, f'{structure_id}.fasta')\n",
    "\n",
    "        # Skip if FASTA already exists\n",
    "        if os.path.exists(fasta_file_path):\n",
    "            print(f\"  ‚è≠Ô∏è  Skipped: {structure_id}.fasta (already exists)\")\n",
    "            fasta_files.append(fasta_file_path)\n",
    "            continue\n",
    "\n",
    "        structure = parser.get_structure(structure_id, pdb_file)\n",
    "\n",
    "        # Extract sequences for each chain\n",
    "        chain_sequences = []\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                chain_id = chain.get_id()\n",
    "                chain_sequence = \"\"\n",
    "                for pp in ppb.build_peptides(chain):\n",
    "                    chain_sequence += str(pp.get_sequence())\n",
    "\n",
    "                if chain_sequence:\n",
    "                    chain_sequences.append((chain_id, chain_sequence))\n",
    "\n",
    "        # Write multi-chain FASTA file\n",
    "        if chain_sequences:\n",
    "            with open(fasta_file_path, 'w') as f:\n",
    "                for chain_id, sequence in chain_sequences:\n",
    "                    f.write(f\">chain_{chain_id}\\n\")\n",
    "                    f.write(f\"{sequence}\\n\")\n",
    "\n",
    "            fasta_files.append(fasta_file_path)\n",
    "            chain_info = \", \".join([f\"{chain_id}({len(seq)} aa)\" for chain_id, seq in chain_sequences])\n",
    "            print(f\"  ‚úì Created: {structure_id}.fasta [{chain_info}]\")\n",
    "\n",
    "    return fasta_files\n",
    "\n",
    "def get_fasta_files(input_dir):\n",
    "    import os\n",
    "    fasta_files = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".fasta\") or file.endswith(\".fa\") or file.endswith(\".fna\"):\n",
    "                fasta_files.append(os.path.join(root, file))\n",
    "    return fasta_files\n",
    "\n",
    "def get_sequence_from_fasta(file_path):\n",
    "    import os\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    sequences = []\n",
    "    current_seq = \"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if current_seq:\n",
    "                    sequences.append(current_seq)\n",
    "                    current_seq = \"\"\n",
    "            else:\n",
    "                current_seq += line\n",
    "        if current_seq:\n",
    "            sequences.append(current_seq)\n",
    "    combined_sequence = ':'.join(sequences)\n",
    "\n",
    "    # Check legit sequence\n",
    "    valid_aa = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    if not all(residue in valid_aa for residue in combined_sequence.replace(\":\", \"\")):\n",
    "        print(f\"Warning: Sequence in {file_path} contains non-standard amino acids. Skipping this file.\")\n",
    "        return {}\n",
    "    else:\n",
    "        return {base_name: combined_sequence}\n",
    "\n",
    "def add_hash(x, y):\n",
    "    import hashlib\n",
    "    return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]\n",
    "\n",
    "def is_sequence_completed(job_dir):\n",
    "    \"\"\"Check if a .done.txt file exists in the job directory\"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(job_dir):\n",
    "        return False\n",
    "    for file in os.listdir(job_dir):\n",
    "        if file.endswith('.done.txt'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_queries_csv_ready(job_dir, jobname):\n",
    "    \"\"\"Check if queries CSV file exists and is valid\"\"\"\n",
    "    import os\n",
    "    queries_path = os.path.join(job_dir, f\"{jobname}.csv\")\n",
    "    if not os.path.exists(queries_path):\n",
    "        return False\n",
    "    # Verify it has content (at least header + 1 line)\n",
    "    try:\n",
    "        with open(queries_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            return len(lines) >= 2 and lines[0].strip() == \"id,sequence\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# Process Input Files\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "from sys import version_info\n",
    "\n",
    "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
    "use_amber = num_relax > 0\n",
    "\n",
    "# Handle PDB input - extract sequences and create FASTA files\n",
    "if input_type == 'pdb':\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 1: Converting PDB files to FASTA format\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    os.system(\"pip install -q --no-warn-conflicts biopython\")\n",
    "    pdb_files = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdb\"):\n",
    "                pdb_files.append(os.path.join(root, file))\n",
    "\n",
    "    if not pdb_files:\n",
    "        raise ValueError(f\"No PDB files found in the directory: {input_dir}\")\n",
    "\n",
    "    print(f\"Found {len(pdb_files)} PDB files in {input_dir}\")\n",
    "    print(\"Extracting sequences and creating FASTA files...\")\n",
    "    fasta_files_created = build_fasta_from_pdb(pdb_files, input_dir)\n",
    "    print(f\"‚úì Processed {len(pdb_files)} PDB files ({len(fasta_files_created)} FASTA files ready)\\n\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 1: Skipping PDB conversion (input_type='fasta')\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Get FASTA files from input directory\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 2: Discovering FASTA files\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fasta_files = get_fasta_files(input_dir)\n",
    "if not fasta_files:\n",
    "    raise ValueError(f\"No FASTA files found in the directory: {input_dir}\")\n",
    "print(f\"‚úì Found {len(fasta_files)} FASTA files in {input_dir}\\n\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process sequences\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 3: Processing sequences and checking completion status\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sequences_dict = {}\n",
    "fasta_file_mapping = {}\n",
    "skipped_sequences = []\n",
    "\n",
    "for fasta_file in fasta_files:\n",
    "    sequence_data = get_sequence_from_fasta(fasta_file)\n",
    "    if sequence_data:\n",
    "        for jobname, sequence in sequence_data.items():\n",
    "            clean_sequence = \"\".join(sequence.split())\n",
    "            base_jobname = \"\".join(jobname.split())\n",
    "            base_jobname = re.sub(r'\\W+', '', base_jobname)\n",
    "            hashed_jobname = add_hash(base_jobname, clean_sequence)\n",
    "\n",
    "            job_dir = os.path.join(output_dir, hashed_jobname)\n",
    "            if is_sequence_completed(job_dir):\n",
    "                print(f\"‚è≠Ô∏è  {jobname} ‚Üí {hashed_jobname} [COMPLETED - has .done.txt]\")\n",
    "                skipped_sequences.append(jobname)\n",
    "            else:\n",
    "                sequences_dict[jobname] = sequence\n",
    "                fasta_file_mapping[jobname] = fasta_file\n",
    "                print(f\"üìã {jobname} ‚Üí {hashed_jobname} [QUEUED]\")\n",
    "\n",
    "print(f\"\\n‚úì Status: {len(sequences_dict)} sequences queued, {len(skipped_sequences)} already completed\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# Prepare Batch Jobs\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 4: Preparing batch job directories and query files\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "batch_jobs = []\n",
    "queries_created = 0\n",
    "queries_skipped = 0\n",
    "\n",
    "for current_jobname, current_query_sequence in sequences_dict.items():\n",
    "    clean_query_sequence = \"\".join(current_query_sequence.split())\n",
    "\n",
    "    base_jobname = \"\".join(current_jobname.split())\n",
    "    base_jobname = re.sub(r'\\W+', '', base_jobname)\n",
    "    hashed_jobname = add_hash(base_jobname, clean_query_sequence)\n",
    "    final_jobname = hashed_jobname\n",
    "\n",
    "    job_dir = os.path.join(output_dir, final_jobname)\n",
    "    os.makedirs(job_dir, exist_ok=True)\n",
    "\n",
    "    queries_path = os.path.join(job_dir, f\"{final_jobname}.csv\")\n",
    "\n",
    "    # Check if queries CSV already exists and is valid\n",
    "    if is_queries_csv_ready(job_dir, final_jobname):\n",
    "        print(f\"  ‚è≠Ô∏è  {final_jobname}.csv (already exists)\")\n",
    "        queries_skipped += 1\n",
    "    else:\n",
    "        # Create queries CSV\n",
    "        with open(queries_path, \"w\") as text_file:\n",
    "            text_file.write(f\"id,sequence\\n{final_jobname},{clean_query_sequence}\")\n",
    "        print(f\"  ‚úì Created {final_jobname}.csv (length: {len(clean_query_sequence.replace(':', ''))} aa)\")\n",
    "        queries_created += 1\n",
    "\n",
    "    fasta_file_path = fasta_file_mapping.get(current_jobname, \"\")\n",
    "    fasta_basename = os.path.splitext(os.path.basename(fasta_file_path))[0] if fasta_file_path else \"\"\n",
    "\n",
    "    batch_jobs.append({\n",
    "        'jobname': final_jobname,\n",
    "        'original_jobname': current_jobname,\n",
    "        'fasta_basename': fasta_basename,\n",
    "        'query_sequence': clean_query_sequence,\n",
    "        'queries_path': queries_path,\n",
    "        'job_dir': job_dir\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úì Query files: {queries_created} created, {queries_skipped} already existed\")\n",
    "print(f\"‚úì Total batch jobs ready: {len(batch_jobs)}\\n\")\n",
    "\n",
    "# Process parameters\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 5: Processing parameters\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
    "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
    "if max_msa == \"auto\":\n",
    "    max_msa = None\n",
    "\n",
    "# Template settings\n",
    "if template_mode == \"pdb100\":\n",
    "    use_templates = True\n",
    "    custom_template_path = None\n",
    "else:\n",
    "    custom_template_path = None\n",
    "    use_templates = False\n",
    "\n",
    "print(f\"  Model type: {model_type}\")\n",
    "print(f\"  MSA mode: {msa_mode}\")\n",
    "print(f\"  Num recycles: {num_recycles if num_recycles is not None else 'auto'}\")\n",
    "print(f\"  Num seeds: {num_seeds}\")\n",
    "print(f\"  Use templates: {use_templates}\")\n",
    "print(f\"  Num relax: {num_relax}\\n\")\n",
    "\n",
    "# Build initial guess mapping if enabled\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 6: Building initial guess mapping\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "initial_guess_mapping = {}\n",
    "if use_initial_guess and initial_guess_dir:\n",
    "    if os.path.exists(initial_guess_dir):\n",
    "        # Collect all PDB files with their basenames\n",
    "        pdb_files = {}\n",
    "        for file in os.listdir(initial_guess_dir):\n",
    "            if file.endswith('.pdb') or file.endswith('.cif'):\n",
    "                basename = os.path.splitext(file)[0]\n",
    "                pdb_files[basename] = os.path.join(initial_guess_dir, file)\n",
    "\n",
    "        print(f\"Found {len(pdb_files)} PDB/CIF files in initial guess directory\")\n",
    "\n",
    "        # Match FASTA files to PDB files using prefix matching\n",
    "        exact_matches = 0\n",
    "        prefix_matches = 0\n",
    "        no_matches = 0\n",
    "\n",
    "        for fasta_file in fasta_files:\n",
    "            fasta_basename = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "            matched = False\n",
    "\n",
    "            # First try exact match\n",
    "            if fasta_basename in pdb_files:\n",
    "                initial_guess_mapping[fasta_basename] = pdb_files[fasta_basename]\n",
    "                print(f\"  ‚úì Exact match: {fasta_basename} ‚Üí {os.path.basename(pdb_files[fasta_basename])}\")\n",
    "                exact_matches += 1\n",
    "                matched = True\n",
    "            else:\n",
    "                # Try prefix matching: find PDB whose name is a prefix of the FASTA name\n",
    "                for pdb_basename, pdb_path in pdb_files.items():\n",
    "                    if fasta_basename.startswith(pdb_basename):\n",
    "                        initial_guess_mapping[fasta_basename] = pdb_path\n",
    "                        print(f\"  ‚úì Prefix match: {fasta_basename} ‚Üí {os.path.basename(pdb_path)}\")\n",
    "                        prefix_matches += 1\n",
    "                        matched = True\n",
    "                        break\n",
    "\n",
    "            if not matched:\n",
    "                print(f\"  ‚ö†Ô∏è  No match: {fasta_basename}\")\n",
    "                no_matches += 1\n",
    "\n",
    "        print(f\"\\n‚úì Initial guess summary:\")\n",
    "        print(f\"  - Exact matches: {exact_matches}\")\n",
    "        print(f\"  - Prefix matches: {prefix_matches}\")\n",
    "        print(f\"  - No matches: {no_matches}\")\n",
    "        print(f\"  - Total mapped: {len(initial_guess_mapping)}/{len(fasta_files)}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Initial guess directory not found: {initial_guess_dir}\")\n",
    "        use_initial_guess = False\n",
    "elif use_initial_guess and not initial_guess_dir:\n",
    "    print(\"‚ö†Ô∏è  Warning: use_initial_guess enabled but no initial_guess_dir provided\")\n",
    "    use_initial_guess = False\n",
    "else:\n",
    "    print(\"Initial guess disabled\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úì CONFIGURATION COMPLETE - Ready to run predictions\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  - Total FASTA files: {len(fasta_files)}\")\n",
    "print(f\"  - Sequences to process: {len(batch_jobs)}\")\n",
    "print(f\"  - Sequences already completed: {len(skipped_sequences)}\")\n",
    "print(f\"  - Initial guess enabled: {use_initial_guess}\")\n",
    "if use_initial_guess:\n",
    "    print(f\"  - Initial guess mappings: {len(initial_guess_mapping)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kOblAo-xetgx"
   },
   "outputs": [],
   "source": [
    "#@title Install All Dependencies\n",
    "#@markdown This cell installs ColabFold, Amber (if needed), and HHsuite (if needed)\n",
    "\n",
    "%%time\n",
    "import os\n",
    "\n",
    "USE_AMBER = use_amber\n",
    "USE_TEMPLATES = use_templates\n",
    "PYTHON_VERSION = python_version\n",
    "\n",
    "# Install ColabFold\n",
    "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
    "    print(\"Installing ColabFold...\")\n",
    "    os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
    "    if os.environ.get('TPU_NAME', False) != False:\n",
    "        os.system(\"pip uninstall -y jax jaxlib\")\n",
    "        os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
    "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
    "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
    "    # Fix TF crash\n",
    "    os.system(\"rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\")\n",
    "    os.system(\"touch COLABFOLD_READY\")\n",
    "    print(\"‚úì ColabFold installed\")\n",
    "\n",
    "# Install Conda if needed\n",
    "if USE_AMBER or USE_TEMPLATES:\n",
    "    if not os.path.isfile(\"CONDA_READY\"):\n",
    "        print(\"Installing Conda...\")\n",
    "        os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
    "        os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
    "        os.system(\"mamba config --set auto_update_conda false\")\n",
    "        os.system(\"touch CONDA_READY\")\n",
    "        print(\"‚úì Conda installed\")\n",
    "\n",
    "# Install HHsuite and/or Amber\n",
    "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "    print(\"Installing HHsuite and Amber...\")\n",
    "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "    os.system(\"touch HH_READY\")\n",
    "    os.system(\"touch AMBER_READY\")\n",
    "    print(\"‚úì HHsuite and Amber installed\")\n",
    "else:\n",
    "    if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
    "        print(\"Installing HHsuite...\")\n",
    "        os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
    "        os.system(\"touch HH_READY\")\n",
    "        print(\"‚úì HHsuite installed\")\n",
    "    if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "        print(\"Installing Amber...\")\n",
    "        os.system(f\"mamba install -y -c conda-forge openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "        os.system(\"touch AMBER_READY\")\n",
    "        print(\"‚úì Amber installed\")\n",
    "\n",
    "print(\"\\n‚úì All dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AzIKiDiCaHAn"
   },
   "outputs": [],
   "source": [
    "#@title Run Prediction\n",
    "display_images = False #@param {type:\"boolean\"}\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from Bio import BiopythonDeprecationWarning\n",
    "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
    "from pathlib import Path\n",
    "from colabfold.download import download_alphafold_params, default_data_dir\n",
    "from colabfold.utils import setup_logging\n",
    "from colabfold.batch import get_queries, run, set_model_type\n",
    "from colabfold.plot import plot_msa_v2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check for K80 GPU\n",
    "try:\n",
    "    K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
    "except:\n",
    "    K80_chk = \"0\"\n",
    "    pass\n",
    "\n",
    "if \"1\" in K80_chk:\n",
    "    print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
    "    if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
    "        del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
    "    if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
    "        del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
    "\n",
    "from colabfold.colabfold import plot_protein\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add pdbfixer to path if using amber\n",
    "if use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
    "    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
    "\n",
    "def input_features_callback(input_features):\n",
    "    if display_images:\n",
    "        plot_msa_v2(input_features)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "def prediction_callback(protein_obj, length, prediction_result, input_features, mode):\n",
    "    model_name, relaxed = mode\n",
    "    if not relaxed:\n",
    "        if display_images:\n",
    "            fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "# Download AlphaFold parameters once for all jobs\n",
    "for i, job in enumerate(batch_jobs):\n",
    "    current_queries_path = job['queries_path']\n",
    "    queries, is_complex = get_queries(current_queries_path)\n",
    "    model_type = set_model_type(is_complex, model_type)\n",
    "    break\n",
    "\n",
    "download_alphafold_params(model_type, Path(\".\"))\n",
    "\n",
    "# Batch processing: run prediction for each sequence\n",
    "print(f\"Starting batch processing for {len(batch_jobs)} sequences...\")\n",
    "batch_results = []\n",
    "\n",
    "for i, job in enumerate(batch_jobs):\n",
    "    current_jobname = job['jobname']\n",
    "    current_query_sequence = job['query_sequence']\n",
    "    current_queries_path = job['queries_path']\n",
    "    current_job_dir = job['job_dir']\n",
    "    fasta_basename = job.get('fasta_basename', '')\n",
    "\n",
    "    print(f\"\\n=== Processing {i+1}/{len(batch_jobs)}: {current_jobname} ===\")\n",
    "\n",
    "    # Setup MSA file path based on mode\n",
    "    if \"mmseqs2\" in msa_mode:\n",
    "        a3m_file = os.path.join(current_job_dir, f\"{current_jobname}.a3m\")\n",
    "    else:  # single_sequence mode\n",
    "        a3m_file = os.path.join(current_job_dir, f\"{current_jobname}.single_sequence.a3m\")\n",
    "        with open(a3m_file, \"w\") as text_file:\n",
    "            text_file.write(f\">1\\n{current_query_sequence}\")\n",
    "\n",
    "    # Setup logging\n",
    "    log_filename = os.path.join(current_job_dir, \"log.txt\")\n",
    "    setup_logging(Path(log_filename))\n",
    "\n",
    "    # Get queries and model type\n",
    "    queries, is_complex = get_queries(current_queries_path)\n",
    "    model_type = set_model_type(is_complex, model_type)\n",
    "\n",
    "    if \"multimer\" in model_type and max_msa is not None:\n",
    "        use_cluster_profile = False\n",
    "    else:\n",
    "        use_cluster_profile = True\n",
    "\n",
    "    # Setup initial guess\n",
    "    current_initial_guess = None\n",
    "    if use_initial_guess and fasta_basename:\n",
    "        if fasta_basename in initial_guess_mapping:\n",
    "            current_initial_guess = initial_guess_mapping[fasta_basename]\n",
    "            print(f\"Using initial guess from: {os.path.basename(current_initial_guess)}\")\n",
    "        else:\n",
    "            print(f\"Warning: No initial guess PDB found for {fasta_basename}\")\n",
    "\n",
    "    # Run prediction\n",
    "    try:\n",
    "        results = run(\n",
    "            queries=queries,\n",
    "            result_dir=current_job_dir,\n",
    "            use_templates=use_templates,\n",
    "            custom_template_path=custom_template_path,\n",
    "            num_relax=num_relax,\n",
    "            msa_mode=msa_mode,\n",
    "            model_type=model_type,\n",
    "            num_models=5,\n",
    "            num_recycles=num_recycles,\n",
    "            relax_max_iterations=relax_max_iterations,\n",
    "            recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
    "            num_seeds=num_seeds,\n",
    "            use_dropout=use_dropout,\n",
    "            model_order=[1,2,3,4,5],\n",
    "            initial_guess=current_initial_guess,\n",
    "            is_complex=is_complex,\n",
    "            data_dir=Path(\".\"),\n",
    "            keep_existing_results=False,\n",
    "            rank_by=\"auto\",\n",
    "            pair_mode=pair_mode,\n",
    "            pairing_strategy=pairing_strategy,\n",
    "            stop_at_score=float(100),\n",
    "            prediction_callback=prediction_callback,\n",
    "            dpi=dpi,\n",
    "            zip_results=False,\n",
    "            save_all=save_all,\n",
    "            max_msa=max_msa,\n",
    "            use_cluster_profile=use_cluster_profile,\n",
    "            input_features_callback=input_features_callback,\n",
    "            save_recycles=save_recycles,\n",
    "            user_agent=\"colabfold/google-colab-main\",\n",
    "            calc_extra_ptm=calc_extra_ptm,\n",
    "        )\n",
    "\n",
    "        # Create result zip for this job\n",
    "        results_zip = os.path.join(current_job_dir, f\"{current_jobname}.result.zip\")\n",
    "        os.system(f\"cd {current_job_dir} && zip -r {current_jobname}.result.zip .\")\n",
    "\n",
    "        batch_results.append({\n",
    "            'jobname': current_jobname,\n",
    "            'results': results,\n",
    "            'zip_path': results_zip,\n",
    "            'job_dir': current_job_dir\n",
    "        })\n",
    "\n",
    "        print(f\"‚úì Completed {current_jobname}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed {current_jobname}: {str(e)}\")\n",
    "        batch_results.append({\n",
    "            'jobname': current_jobname,\n",
    "            'results': None,\n",
    "            'zip_path': None,\n",
    "            'job_dir': current_job_dir,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "successful_count = len([r for r in batch_results if r.get('results')])\n",
    "failed_count = len([r for r in batch_results if r.get('error')])\n",
    "print(f\"\\nBatch processing completed: {successful_count} successful, {failed_count} failed\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
